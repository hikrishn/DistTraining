{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-ignite\n",
      "  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.1/264.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (2.4.1)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (4.47.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch<2,>=1.3\n",
      "  Downloading torch-1.12.1-cp38-none-macosx_10_9_x86_64.whl (137.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.8/137.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from pytorch-ignite) (20.4)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (3.15.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (1.28.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (2.24.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (0.12.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (49.2.0.post20200714)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (0.4.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (1.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (1.32.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from tensorboard) (1.19.5)\n",
      "Requirement already satisfied: six in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from fire) (1.15.0)\n",
      "Requirement already satisfied: termcolor in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from fire) (1.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied: typing-extensions in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from packaging->pytorch-ignite) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=a414168e42a7a0a24e8f1dd00589f341532ec88d1cf6219d9d0f3a3f925fcd21\n",
      "  Stored in directory: /private/var/folders/d9/_j0t97d15hvgxt0lnyq1500h0000gn/T/pip-ephem-wheel-cache-7aki602z/wheels/1f/10/06/2a990ee4d73a8479fe2922445e8a876d38cfbfed052284c6a1\n",
      "Successfully built fire\n",
      "Installing collected packages: tqdm, torch, tensorboard-data-server, fire, pytorch-ignite, tensorboard\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.47.0\n",
      "    Uninstalling tqdm-4.47.0:\n",
      "      Successfully uninstalled tqdm-4.47.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.4.1\n",
      "    Uninstalling tensorboard-2.4.1:\n",
      "      Successfully uninstalled tensorboard-2.4.1\n",
      "Successfully installed fire-0.4.0 pytorch-ignite-0.4.10 tensorboard-2.10.1 tensorboard-data-server-0.6.1 torch-1.12.1 tqdm-4.64.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-cache-dir pytorch-ignite \\\n",
    "                                         tensorboard \\\n",
    "                                         tqdm \\\n",
    "                                         fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-22.3-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.2\n",
      "    Uninstalling pip-22.2.2:\n",
      "      Successfully uninstalled pip-22.2.2\n",
      "Successfully installed pip-22.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (1.12.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.13.1-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-0.12.1-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: numpy in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: requests in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from torchvision) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (2.10)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-0.12.1 torchvision-0.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mERROR: \u001b[0mCannot find key: -f\n",
      "Usage: ipykernel_launcher.py <command>\n",
      "  available commands:    run\n",
      "\n",
      "For detailed information on this command, run:\n",
      "  ipykernel_launcher.py --help\n"
     ]
    },
    {
     "ename": "FireExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mFireExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishna/Creative_Man/AI_ML/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    Pad,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "import ignite\n",
    "import ignite.distributed as idist\n",
    "from ignite.contrib.engines import common\n",
    "from ignite.handlers import PiecewiseLinear\n",
    "from ignite.engine import (\n",
    "    Events,\n",
    "    create_supervised_trainer,\n",
    "    create_supervised_evaluator,\n",
    ")\n",
    "from ignite.handlers import Checkpoint, global_step_from_engine\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.utils import manual_seed, setup_logger\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"seed\": 543,\n",
    "    \"data_path\": \"cifar10\",\n",
    "    \"output_path\": \"output-cifar10/\",\n",
    "    \"model\": \"resnet18\",\n",
    "    \"batch_size\": 512,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"num_workers\": 2,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 0.4,\n",
    "    \"num_warmup_epochs\": 1,\n",
    "    \"validate_every\": 3,\n",
    "    \"checkpoint_every\": 200,\n",
    "    \"backend\": None,\n",
    "    \"resume_from\": None,\n",
    "    \"log_every_iters\": 15,\n",
    "    \"nproc_per_node\": None,\n",
    "    \"with_clearml\": False,\n",
    "    \"with_amp\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def get_train_test_datasets(path):\n",
    "    train_transform = Compose(\n",
    "        [\n",
    "            Pad(4),\n",
    "            RandomCrop(32, fill=128),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "    test_transform = Compose(\n",
    "        [\n",
    "            ToTensor(),\n",
    "            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_ds = datasets.CIFAR10(\n",
    "        root=path, train=True, download=False, transform=train_transform\n",
    "    )\n",
    "    test_ds = datasets.CIFAR10(\n",
    "        root=path, train=False, download=False, transform=test_transform\n",
    "    )\n",
    "\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "def get_dataflow(config):\n",
    "    train_dataset, test_dataset = get_train_test_datasets(config[\"data_path\"])\n",
    "\n",
    "    train_loader = idist.auto_dataloader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    test_loader = idist.auto_dataloader(\n",
    "        test_dataset,\n",
    "        batch_size=2 * config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def get_model(config):\n",
    "    model_name = config[\"model\"]\n",
    "    if model_name in models.__dict__:\n",
    "        fn = models.__dict__[model_name]\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unknown model name {model_name}\")\n",
    "\n",
    "    model = idist.auto_model(fn(num_classes=10))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_optimizer(config, model):\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config[\"learning_rate\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        nesterov=True,\n",
    "    )\n",
    "    optimizer = idist.auto_optim(optimizer)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_criterion():\n",
    "    return nn.CrossEntropyLoss().to(idist.device())\n",
    "\n",
    "\n",
    "def get_lr_scheduler(config, optimizer):\n",
    "    milestones_values = [\n",
    "        (0, 0.0),\n",
    "        (\n",
    "            config[\"num_iters_per_epoch\"] * config[\"num_warmup_epochs\"],\n",
    "            config[\"learning_rate\"],\n",
    "        ),\n",
    "        (config[\"num_iters_per_epoch\"] * config[\"num_epochs\"], 0.0),\n",
    "    ]\n",
    "    lr_scheduler = PiecewiseLinear(\n",
    "        optimizer, param_name=\"lr\", milestones_values=milestones_values\n",
    "    )\n",
    "    return lr_scheduler\n",
    "\n",
    "\n",
    "def get_save_handler(config):\n",
    "    if config[\"with_clearml\"]:\n",
    "        from ignite.contrib.handlers.clearml_logger import ClearMLSaver\n",
    "\n",
    "        return ClearMLSaver(dirname=config[\"output_path\"])\n",
    "\n",
    "    return config[\"output_path\"]\n",
    "\n",
    "\n",
    "def load_checkpoint(resume_from):\n",
    "    checkpoint_fp = Path(resume_from)\n",
    "    assert (\n",
    "        checkpoint_fp.exists()\n",
    "    ), f\"Checkpoint '{checkpoint_fp.as_posix()}' is not found\"\n",
    "    checkpoint = torch.load(checkpoint_fp.as_posix(), map_location=\"cpu\")\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def create_trainer(\n",
    "    model, optimizer, criterion, lr_scheduler, train_sampler, config, logger\n",
    "):\n",
    "\n",
    "    device = idist.device()\n",
    "    amp_mode = None\n",
    "    scaler = False\n",
    "\n",
    "    trainer = create_supervised_trainer(\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device=device,\n",
    "        non_blocking=True,\n",
    "        output_transform=lambda x, y, y_pred, loss: {\"batch loss\": loss.item()},\n",
    "        amp_mode=\"amp\" if config[\"with_amp\"] else None,\n",
    "        scaler=config[\"with_amp\"],\n",
    "    )\n",
    "    trainer.logger = logger\n",
    "\n",
    "    to_save = {\n",
    "        \"trainer\": trainer,\n",
    "        \"model\": model,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": lr_scheduler,\n",
    "    }\n",
    "    metric_names = [\n",
    "        \"batch loss\",\n",
    "    ]\n",
    "\n",
    "    common.setup_common_training_handlers(\n",
    "        trainer=trainer,\n",
    "        train_sampler=train_sampler,\n",
    "        to_save=to_save,\n",
    "        save_every_iters=config[\"checkpoint_every\"],\n",
    "        save_handler=get_save_handler(config),\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        output_names=metric_names if config[\"log_every_iters\"] > 0 else None,\n",
    "        with_pbars=False,\n",
    "        clear_cuda_cache=False,\n",
    "    )\n",
    "\n",
    "    if config[\"resume_from\"] is not None:\n",
    "        checkpoint = load_checkpoint(config[\"resume_from\"])\n",
    "        Checkpoint.load_objects(to_load=to_save, checkpoint=checkpoint)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def create_evaluator(model, metrics, config):\n",
    "    device = idist.device()\n",
    "\n",
    "    amp_mode = \"amp\" if config[\"with_amp\"] else None\n",
    "    evaluator = create_supervised_evaluator(\n",
    "        model, metrics=metrics, device=device, non_blocking=True, amp_mode=amp_mode\n",
    "    )\n",
    "\n",
    "    return evaluator\n",
    "\n",
    "\n",
    "def setup_rank_zero(logger, config):\n",
    "    device = idist.device()\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_path = config[\"output_path\"]\n",
    "    folder_name = (\n",
    "        f\"{config['model']}_backend-{idist.backend()}-{idist.get_world_size()}_{now}\"\n",
    "    )\n",
    "    output_path = Path(output_path) / folder_name\n",
    "    if not output_path.exists():\n",
    "        output_path.mkdir(parents=True)\n",
    "    config[\"output_path\"] = output_path.as_posix()\n",
    "    logger.info(f\"Output path: {config['output_path']}\")\n",
    "\n",
    "    if config[\"with_clearml\"]:\n",
    "        from clearml import Task\n",
    "\n",
    "        task = Task.init(\"CIFAR10-Training\", task_name=output_path.stem)\n",
    "        task.connect_configuration(config)\n",
    "        # Log hyper parameters\n",
    "        hyper_params = [\n",
    "            \"model\",\n",
    "            \"batch_size\",\n",
    "            \"momentum\",\n",
    "            \"weight_decay\",\n",
    "            \"num_epochs\",\n",
    "            \"learning_rate\",\n",
    "            \"num_warmup_epochs\",\n",
    "        ]\n",
    "        task.connect({k: v for k, v in config.items()})\n",
    "\n",
    "\n",
    "def log_basic_info(logger, config):\n",
    "    logger.info(f\"Train on CIFAR10\")\n",
    "    logger.info(f\"- PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"- Ignite version: {ignite.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        # explicitly import cudnn as torch.backends.cudnn can not be pickled with hvd spawning procs\n",
    "        from torch.backends import cudnn\n",
    "\n",
    "        logger.info(\n",
    "            f\"- GPU Device: {torch.cuda.get_device_name(idist.get_local_rank())}\"\n",
    "        )\n",
    "        logger.info(f\"- CUDA version: {torch.version.cuda}\")\n",
    "        logger.info(f\"- CUDNN version: {cudnn.version()}\")\n",
    "\n",
    "    logger.info(\"\\n\")\n",
    "    logger.info(\"Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        logger.info(f\"\\t{key}: {value}\")\n",
    "    logger.info(\"\\n\")\n",
    "\n",
    "    if idist.get_world_size() > 1:\n",
    "        logger.info(\"\\nDistributed setting:\")\n",
    "        logger.info(f\"\\tbackend: {idist.backend()}\")\n",
    "        logger.info(f\"\\tworld size: {idist.get_world_size()}\")\n",
    "        logger.info(\"\\n\")\n",
    "\n",
    "\n",
    "def log_metrics(logger, epoch, elapsed, tag, metrics):\n",
    "    metrics_output = \"\\n\".join([f\"\\t{k}: {v}\" for k, v in metrics.items()])\n",
    "    logger.info(\n",
    "        f\"\\nEpoch {epoch} - Evaluation time (seconds): {elapsed:.2f} - {tag} metrics:\\n {metrics_output}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def training(local_rank, config):\n",
    "\n",
    "    rank = idist.get_rank()\n",
    "    manual_seed(config[\"seed\"] + rank)\n",
    "\n",
    "    logger = setup_logger(name=\"CIFAR10-Training\")\n",
    "    log_basic_info(logger, config)\n",
    "\n",
    "    if rank == 0:\n",
    "        setup_rank_zero(logger, config)\n",
    "\n",
    "    train_loader, val_loader = get_dataflow(config)\n",
    "    model = get_model(config)\n",
    "    optimizer = get_optimizer(config, model)\n",
    "    criterion = get_criterion()\n",
    "    config[\"num_iters_per_epoch\"] = len(train_loader)\n",
    "    lr_scheduler = get_lr_scheduler(config, optimizer)\n",
    "\n",
    "    trainer = create_trainer(\n",
    "        model, optimizer, criterion, lr_scheduler, train_loader.sampler, config, logger\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": Accuracy(),\n",
    "        \"Loss\": Loss(criterion),\n",
    "    }\n",
    "\n",
    "    train_evaluator = create_evaluator(model, metrics, config)\n",
    "    val_evaluator = create_evaluator(model, metrics, config)\n",
    "\n",
    "    def run_validation(engine):\n",
    "        epoch = trainer.state.epoch\n",
    "        state = train_evaluator.run(train_loader)\n",
    "        log_metrics(logger, epoch, state.times[\"COMPLETED\"], \"train\", state.metrics)\n",
    "        state = val_evaluator.run(val_loader)\n",
    "        log_metrics(logger, epoch, state.times[\"COMPLETED\"], \"val\", state.metrics)\n",
    "\n",
    "    trainer.add_event_handler(\n",
    "        Events.EPOCH_COMPLETED(every=config[\"validate_every\"]) | Events.COMPLETED,\n",
    "        run_validation,\n",
    "    )\n",
    "\n",
    "    if rank == 0:\n",
    "        evaluators = {\"train\": train_evaluator, \"val\": val_evaluator}\n",
    "        tb_logger = common.setup_tb_logging(\n",
    "            config[\"output_path\"], trainer, optimizer, evaluators=evaluators\n",
    "        )\n",
    "\n",
    "    best_model_handler = Checkpoint(\n",
    "        {\"model\": model},\n",
    "        get_save_handler(config),\n",
    "        filename_prefix=\"best\",\n",
    "        n_saved=2,\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "        score_name=\"val_accuracy\",\n",
    "        score_function=Checkpoint.get_default_score_fn(\"Accuracy\"),\n",
    "    )\n",
    "    val_evaluator.add_event_handler(\n",
    "        Events.COMPLETED,\n",
    "        best_model_handler,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.run(train_loader, max_epochs=config[\"num_epochs\"])\n",
    "    except Exception as e:\n",
    "        logger.exception(\"\")\n",
    "        raise e\n",
    "\n",
    "    if rank == 0:\n",
    "        tb_logger.close()\n",
    "\n",
    "## For running on GPU Instances\n",
    "#def run(backend=None, **spawn_kwargs):\n",
    "#    config[\"backend\"] = backend\n",
    "\n",
    "#    with idist.Parallel(backend=config[\"backend\"], **spawn_kwargs) as parallel:\n",
    "#        parallel.run(training, config)\n",
    "\n",
    "## For running on Google tpu\n",
    "def run(backend=None, **spawn_kwargs):\n",
    "    nproc_per_node = 8\n",
    "    config[\"backend\"] = \"xla-tpu\"\n",
    "\n",
    "    with idist.Parallel(backend=config[\"backend\"], nproc_per_node=nproc_per_node) as parallel:\n",
    "        parallel.run(training, config)\n",
    "\n",
    "## For running on Jupyter Notebook\n",
    "def run(backend=None, **spawn_kwargs):\n",
    "    spawn_kwargs = {}\n",
    "    spawn_kwargs[\"start_method\"] = \"fork\"\n",
    "    spawn_kwargs[\"nproc_per_node\"] = 2\n",
    "    config[\"backend\"] = \"nccl\"\n",
    "\n",
    "    with idist.Parallel(backend=config[\"backend\"], **spawn_kwargs) as parallel:\n",
    "        parallel.run(training, config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire({\"run\": run})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
